#!/bin/bash
# ================================================================
#  S3 ì—°ë™ ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸
#  LocalStackì„ S3 ì—ë®¬ë ˆì´í„°ë¡œ ì‚¬ìš©í•˜ì—¬
#  ì‹¤ì œ AWS EMR í™˜ê²½ê³¼ ë™ì¼í•œ S3 I/O íë¦„ì„ ì¬í˜„í•©ë‹ˆë‹¤.
#
#  ì‚¬ìš©ë²•:
#    bash deploy/run_pipeline_s3.sh
# ================================================================

set -e

PROJECT_DIR="$(cd "$(dirname "$0")/.." && pwd)"
cd "$PROJECT_DIR"

# -- S3 í™˜ê²½ë³€ìˆ˜ (LocalStack) --
export S3_ENDPOINT="http://localhost:4566"
export AWS_ACCESS_KEY_ID="test"
export AWS_SECRET_ACCESS_KEY="test"
export AWS_DEFAULT_REGION="ap-northeast-2"

# -- Python / Spark ê²½ë¡œ --
export PYSPARK_PYTHON="${PROJECT_DIR}/.venv/bin/python"
export PYSPARK_DRIVER_PYTHON="${PROJECT_DIR}/.venv/bin/python"

# spark-submit ìë™ ê°ì§€
SPARK_SUBMIT=$(find "${PROJECT_DIR}/.venv" -name "spark-submit" -type f 2>/dev/null | head -1)
if [ -z "$SPARK_SUBMIT" ]; then
    echo "âŒ spark-submitì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
    exit 1
fi

# Hadoop-AWS íŒ¨í‚¤ì§€ (PySpark Hadoop 3.4.2ì— ë§ì¶¤)
PACKAGES="org.apache.hadoop:hadoop-aws:3.4.2"

# S3 ê²½ë¡œ
INPUT="s3a://card-pipeline-input/card_transactions.csv"
OUTPUT="s3a://card-pipeline-output"

echo "================================================================"
echo "  ğŸš€ S3 ì—°ë™ ì¹´ë“œ ê±°ë˜ ë°°ì¹˜ íŒŒì´í”„ë¼ì¸"
echo "================================================================"
echo "  Storage  : S3 (LocalStack @ ${S3_ENDPOINT})"
echo "  Input    : ${INPUT}"
echo "  Output   : ${OUTPUT}"
echo "  Packages : hadoop-aws:3.4.2"
echo "================================================================"

TOTAL_START=$(date +%s)

# ============================================
#  Step 1: ë°ì´í„° í’ˆì§ˆ ê²€ì¦
# ============================================
echo ""
echo "============================================"
echo "  [Step 1/4] ë°ì´í„° í’ˆì§ˆ ê²€ì¦"
echo "============================================"
$SPARK_SUBMIT \
    --packages "$PACKAGES" \
    scripts/data_quality_check.py \
        --input "$INPUT" \
        --output "${OUTPUT}/quality_report"

# ============================================
#  Step 2: ë©”ì¸ Spark ETL
# ============================================
echo ""
echo "============================================"
echo "  [Step 2/4] ë©”ì¸ Spark ETL"
echo "============================================"
$SPARK_SUBMIT \
    --packages "$PACKAGES" \
    scripts/spark_etl.py \
        --input "$INPUT" \
        --output "${OUTPUT}"

# ============================================
#  Step 3: ë¶„ê¸°ë³„ ë°°ì¹˜ ì²˜ë¦¬
# ============================================
echo ""
echo "============================================"
echo "  [Step 3/4] ë¶„ê¸°ë³„ ë°°ì¹˜ ì²˜ë¦¬"
echo "============================================"
$SPARK_SUBMIT \
    --packages "$PACKAGES" \
    scripts/quarterly_batch.py \
        --input "$INPUT" \
        --output "${OUTPUT}/quarterly"

# ============================================
#  Step 4: ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬
# ============================================
echo ""
echo "============================================"
echo "  [Step 4/4] ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬"
echo "============================================"
$SPARK_SUBMIT \
    --packages "$PACKAGES" \
    scripts/performance_optimizer.py \
        --input "$INPUT" \
        --output "${OUTPUT}/optimized"

TOTAL_END=$(date +%s)
TOTAL_ELAPSED=$((TOTAL_END - TOTAL_START))

# ============================================
#  ê²°ê³¼ í™•ì¸: S3 ì¶œë ¥ ëª©ë¡
# ============================================
echo ""
echo "================================================================"
echo "  âœ… ì „ì²´ íŒŒì´í”„ë¼ì¸ ì™„ë£Œ! (${TOTAL_ELAPSED}ì´ˆ)"
echo "================================================================"
echo ""
echo "  S3 ì¶œë ¥ ê²°ê³¼:"

"${PROJECT_DIR}/.venv/bin/python" << 'PYEOF'
import boto3, os
s3 = boto3.client("s3",
    endpoint_url=os.getenv("S3_ENDPOINT"),
    aws_access_key_id="test", aws_secret_access_key="test",
    region_name="ap-northeast-2")

paginator = s3.get_paginator("list_objects_v2")
total_size = 0
dirs = set()
file_count = 0

for page in paginator.paginate(Bucket="card-pipeline-output"):
    for obj in page.get("Contents", []):
        parts = obj["Key"].split("/")
        if len(parts) > 1:
            dirs.add(parts[0])
        total_size += obj["Size"]
        file_count += 1

for d in sorted(dirs):
    print(f"    ğŸ“ s3://card-pipeline-output/{d}/")
print(f"\n    ì´ {file_count}ê°œ íŒŒì¼, {total_size / 1024 / 1024:.1f} MB")
PYEOF

echo ""
echo "================================================================"
